import os, sys
ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__),  ".."))
sys.path.insert(0, ROOT)
import numpy as np
from sklearn.neighbors import NearestNeighbors
from synth.timegan_train import train_timegan  # âœ… åªä¿ç•™ä¸€æ¬¡
import pandas as pd
import numpy as np
import random
import csv
import torch
from torch.utils.data import DataLoader, TensorDataset
from sklearn.neighbors import NearestNeighbors

# ========= å¯è°ƒï¼šå¿«é€ŸéªŒè¯å¼€å…³ =========
FRACTION = 1.0   # ä¾‹å¦‚ 0.2 è¡¨ç¤ºåªå– 20% loan åŠ é€Ÿé¢„å¤„ç†
TG_EPOCHS = 1200 # TimeGAN è®­ç»ƒè½®æ•°ï¼ˆå¿«é€ŸéªŒè¯å¯è®¾ 600~1200ï¼›æ­£å¼è·‘ 3000+ï¼‰
TG_H = 96
TG_Z = 24
TG_BATCH = 64
TG_LR = 5e-4
TG_PRINT_EVERY = 200
SEED = 42
# =====================================

# =====================================
# Step 1: Load and group data by loan ID
# =====================================
def load_and_process_data(input_file, fraction=1.0, seed=42):
    df = pd.read_csv(input_file)
    unique_loans = df['LOAN SEQUENCE NUMBER'].unique()

    if 0 < fraction < 1.0:
        np.random.seed(seed)
        sampled_loans = np.random.choice(unique_loans, size=int(len(unique_loans) * fraction), replace=False)
        df = df[df['LOAN SEQUENCE NUMBER'].isin(sampled_loans)]

    sorted_data = df.sort_values('LOAN SEQUENCE NUMBER')
    grouped_data = sorted_data.groupby('LOAN SEQUENCE NUMBER')
    return grouped_data, df


# =====================================
# Step 2: Build base train data
# =====================================
def build_train_data(grouped_data, master_data_size):
    train_data_X = []
    train_data_Y = []
    predict_month = 3
    all_group_sizes = []
    num_defaults = 0
    total_rows_all_groups = 0

    for _, group in grouped_data:
        group = group.sort_values('REMAINING MONTHS TO LEGAL MATURITY', ascending=False)
        group_size = len(group)
        total_rows_all_groups += group_size
        all_group_sizes.append(group_size)

        # æœ€åä¸€åˆ—ä¸ºæ ‡ç­¾
        if group.iloc[:, -1].sum() > 0:
            num_defaults += 1

        if group_size >= master_data_size + predict_month:
            # å»æ‰ç¬¬ä¸€åˆ—IDï¼Œæœ€åä¸€åˆ—æ ‡ç­¾
            X = group.iloc[0:master_data_size, 1:-1].copy()
            Y = group.iloc[master_data_size:master_data_size + predict_month, -1]

            # åˆ é™¤ä¸å¸Œæœ›ä½œä¸ºç‰¹å¾çš„åˆ—ï¼ˆè‹¥å­˜åœ¨ï¼‰
            for col in ['ZERO BALANCE CODE', 'REMAINING MONTHS TO LEGAL MATURITY']:
                if col in X.columns:
                    X.drop(columns=col, inplace=True)

            # âœ… å¼ºåˆ¶æ•°å€¼ & å¡«ç¼ºï¼Œé¿å…å­—ç¬¦ä¸²/ç©ºå€¼å¯¼è‡´è®­ç»ƒæŠ¥é”™
            X = X.apply(pd.to_numeric, errors='coerce').fillna(0.0)

            label = 1 if Y.sum() > 0 else 0
            train_data_X.append(X.values)  # shape (T, D)
            train_data_Y.append(label)

    return (
        np.array(train_data_X),   # (N, T, D)
        np.array(train_data_Y),   # (N,)
        total_rows_all_groups,
        len(all_group_sizes),
        sum(all_group_sizes),
        num_defaults
    )


# =====================================
# Step 3: Sampling Methods
# =====================================

def _print_class_info(prefix, Y):
    pos = int(np.sum(Y == 1))
    neg = int(np.sum(Y == 0))
    total = len(Y)
    rate = pos / total if total > 0 else 0.0
    print(f"{prefix} | total={total}, pos={pos}, neg={neg}, pos_rate={rate:.4f}")


def apply_tsmote_ts(
    X, Y,
    k_neighbors=5,
    n_synthetic=None,            # ç”Ÿæˆå¤šå°‘å°‘æ•°ç±»æ ·æœ¬ï¼›é»˜è®¤è¡¥é½åˆ°ä¸å¤šæ•°ç±»ç›¸åŒ
    alpha_mode="global",         # "global"ï¼šæ•´æ®µåŒä¸€ä¸ªalphaï¼›"per_timestep"ï¼šæ¯ä¸ªtä¸€ä¸ªalpha
    noise_std=0.0,               # å¯é€‰ï¼šå°é«˜æ–¯å™ªå£° Ïƒï¼ˆå»ºè®®0~0.01ä¹‹é—´ï¼Œæ•°æ®å·²æ ‡å‡†åŒ–æ—¶ï¼‰
    smooth=False,                # å¯é€‰ï¼šå¯¹åˆæˆåºåˆ—åšä¸€æ¬¡è½»å¾®å¹³æ»‘
    random_state=42,
    metric="euclidean"
):
    """
    æ—¶åºSMOTEï¼ˆé•¿åº¦Tã€ç»´åº¦Dä¸€è‡´ï¼‰ï¼š
    1) ä»…åœ¨å°‘æ•°ç±»å†…éƒ¨åškNNï¼ˆåŸºäºæ‰å¹³åçš„è·ç¦»ï¼‰é€‰é‚»å±…
    2) å¯¹æ•´æ®µåºåˆ—é€æ—¶åˆ»çº¿æ€§æ’å€¼ï¼šx_syn[t] = (1-Î±)*x_i[t] + Î±*x_j[t]
       - alpha_mode="global": æ•´æ®µä¸€ä¸ªÎ±
       - alpha_mode="per_timestep": æ¯ä¸ªtå•ç‹¬é‡‡æ ·Î±_tï¼Œæ›´æœ‰éšæœºæ€§
    3) å¯é€‰ï¼šåŠ æå°ç™½å™ªå£°ã€ç®€å•å¹³æ»‘
    """
    rng = np.random.default_rng(random_state)
    X = np.asarray(X)
    Y = np.asarray(Y)

    X_min = X[Y == 1]
    X_maj = X[Y == 0]
    n_min, n_maj = len(X_min), len(X_maj)
    if n_min == 0:
        print("âŒ å°‘æ•°ç±»ä¸º0ï¼Œæ— æ³•åšSMOTEã€‚")
        return X, Y

    # éœ€è¦ç”Ÿæˆçš„æ•°é‡
    if n_synthetic is None:
        n_synthetic = n_maj - n_min
    if n_synthetic <= 0:
        print("â„¹ï¸ å·²å¹³è¡¡æˆ–å°‘æ•°ç±»ä¸å°‘ï¼Œè·³è¿‡æ—¶åºSMOTEã€‚")
        return X, Y

    # è¿‘é‚»åªåœ¨å°‘æ•°ç±»å†…éƒ¨
    k_use = min(k_neighbors + 1, n_min)  # åŒ…å«è‡ªèº«
    if k_use <= 1:
        print("âš ï¸ å°‘æ•°ç±»æ ·æœ¬ < 2ï¼Œæ— æ³•kNNæ’å€¼ã€‚")
        return X, Y

    N, T, D = X.shape
    X_min_flat = X_min.reshape(n_min, T * D)
    nn = NearestNeighbors(n_neighbors=k_use, metric=metric)
    nn.fit(X_min_flat)
    neigh_idx = nn.kneighbors(X_min_flat, return_distance=False)  # ç¬¬0åˆ—æ˜¯è‡ªå·±

    syn_list = []
    for _ in range(n_synthetic):
        i = rng.integers(0, n_min)
        # é€‰ä¸€ä¸ªé‚»å±…ï¼ˆæ’é™¤è‡ªå·±ï¼‰
        j = i if neigh_idx.shape[1] == 1 else rng.choice(neigh_idx[i][1:])

        xi = X_min[i]        # (T, D)
        xj = X_min[j]        # (T, D)

        if alpha_mode == "per_timestep":
            # æ¯ä¸ªtä¸€ä¸ªalpha_t
            alpha = rng.random((T, 1))   # å¹¿æ’­åˆ°D
        else:
            # æ•´æ®µä¸€ä¸ªalpha
            a = float(rng.random())
            alpha = np.full((T, 1), a, dtype=xi.dtype)

        x_syn = xi + alpha * (xj - xi)   # é€æ—¶åˆ»çº¿æ€§æ’å€¼ (T,D)

        if noise_std and noise_std > 0:
            x_syn = x_syn + rng.normal(0.0, noise_std, size=x_syn.shape)

        if smooth:
            # ç®€å•æ—¶åŸŸå¹³æ»‘ï¼ˆ3ç‚¹å¹³å‡æ»¤æ³¢ï¼‰ï¼Œä¸å¼•å…¥æœªæ¥ä¿¡æ¯
            x_pad = np.pad(x_syn, ((1,1),(0,0)), mode='edge')  # pad time ç»´
            x_syn = (x_pad[:-2] + x_pad[1:-1] + x_pad[2:]) / 3.0

        syn_list.append(x_syn)

    X_syn = np.stack(syn_list, axis=0)             # (n_synthetic, T, D)
    Y_syn = np.ones((n_synthetic,), dtype=Y.dtype) # éƒ½æ˜¯å°‘æ•°ç±»

    X_aug = np.concatenate([X, X_syn], axis=0)
    Y_aug = np.concatenate([Y, Y_syn], axis=0)
    return X_aug, Y_aug

def generate_timegan_samples(X, Y, num_to_generate=None,
                             epochs=TG_EPOCHS, lr=TG_LR, Z=TG_Z, H=TG_H, batch=TG_BATCH, random_state=SEED):
    """
    ç”¨å°‘æ•°ç±»æ ·æœ¬è®­ç»ƒä¸€ä¸ªæç®€ TimeGANï¼Œç„¶åé‡‡æ ·è¡¥é½åˆ°ä¸å¤šæ•°ç±»ç›¸åŒæ•°é‡ï¼ˆæˆ–è‡ªå®šä¹‰ num_to_generateï¼‰ã€‚
    - X: (N, T, D)  å·²ç»å®Œæˆä½ é¡¹ç›®é‡Œçš„é¢„å¤„ç†/ç¼©æ”¾
    - Y: (N,)
    """
    np.random.seed(random_state)
    torch.manual_seed(random_state)

    # 1) æ‹†åˆ†å°‘æ•°/å¤šæ•°
    X_min = X[Y == 1]
    X_maj = X[Y == 0]
    n_min, n_maj = len(X_min), len(X_maj)
    if n_min == 0:
        print("âŒ å°‘æ•°ç±»ä¸º 0ï¼Œæ— æ³•è®­ç»ƒ TimeGAN")
        return X, Y

    if num_to_generate is None:
        num_to_generate = n_maj - n_min
    if num_to_generate <= 0:
        print("âš ï¸ æ— éœ€ TimeGAN é‡‡æ ·ï¼Œå°‘æ•°ç±»å·²è¶³å¤Ÿ")
        return X, Y

    # 2) ç»„å»º DataLoaderï¼ˆåªç”¨å°‘æ•°ç±»è®­ç»ƒ TimeGANï¼‰
    N, T, D = X.shape
    tensor_min = torch.tensor(X_min, dtype=torch.float32)
    ds = TensorDataset(tensor_min)
    if len(ds) < 2:
        print("âš ï¸ å°‘æ•°ç±»æ ·æœ¬è¿‡å°‘ï¼ˆ<2ï¼‰ï¼Œè·³è¿‡ TimeGANã€‚")
        return X, Y
    batch = min(batch, len(ds))
    dl = DataLoader(ds, batch_size=batch, shuffle=True, drop_last=len(ds) >= batch)

    # 3) è®­ç»ƒ TimeGANï¼ˆä½ é¡¹ç›®é‡Œçš„æç®€å®ç°ï¼‰
    model = train_timegan((b for (b,) in dl), D=D, T=T,
                          epochs=epochs, lr=lr, Z=Z, H=H, print_every=TG_PRINT_EVERY)

    # 4) é‡‡æ ·éœ€è¦çš„æ•°é‡ï¼ˆè·Ÿéšæ¨¡å‹è®¾å¤‡ï¼‰
    device = next(model.parameters()).device
    with torch.no_grad():
        x_fake = model.synthesize(B=num_to_generate, T=T, device=str(device)).cpu().numpy()

    y_fake = np.ones((num_to_generate,), dtype=Y.dtype)

    # 5) åˆå¹¶å›å»
    X_aug = np.concatenate([X, x_fake], axis=0)
    Y_aug = np.concatenate([Y, y_fake], axis=0)
    return X_aug, Y_aug


def apply_sampling(X, Y, method, random_state=SEED):
    """
    å››ç§æ–¹æ³•ï¼š
    - base: ä¸åŠ¨
    - random_undersample: æ¬ é‡‡åˆ°ä¸æ­£æ ·æœ¬åŒæ•°
    - tsmote: å…ˆæ¬ é‡‡å¤šæ•°ç±»åˆ° 1:2ï¼Œå†å¯¹å°‘æ•°ç±»ç”¨ SMOTE è¡¥é½
    - timegan: å…ˆæ¬ é‡‡å¤šæ•°ç±»åˆ° 1:2ï¼Œå†ç”¨ TimeGAN è¡¥é½
    """
    rng = np.random.default_rng(random_state)

    X = np.asarray(X)
    Y = np.asarray(Y)

    pos_idx = np.where(Y == 1)[0]
    neg_idx = np.where(Y == 0)[0]

    if method == "base":
        _print_class_info("BASE before", Y)
        return X, Y

    elif method == "random_undersample":
        # æ¬ é‡‡å¤šæ•°ç±»åˆ°ä¸å°‘æ•°ç±»åŒæ•°
        if len(neg_idx) > len(pos_idx):
            keep_neg = rng.choice(neg_idx, size=len(pos_idx), replace=False)
        else:
            keep_neg = neg_idx
        keep_idx = np.concatenate([pos_idx, keep_neg])
        rng.shuffle(keep_idx)
        X_new, Y_new = X[keep_idx], Y[keep_idx]
        _print_class_info("RUS after", Y_new)
        return X_new, Y_new

    elif method == "tsmote":
        # â‘  æ¬ é‡‡å¤šæ•°ç±»åˆ° 1:2
        if len(neg_idx) > 2 * len(pos_idx):
            keep_neg = rng.choice(neg_idx, size=2 * len(pos_idx), replace=False)
        else:
            keep_neg = neg_idx
        keep_idx = np.concatenate([pos_idx, keep_neg])
        X_part, Y_part = X[keep_idx], Y[keep_idx]
        _print_class_info("tSMOTE step1 (after 1:2 undersample)", Y_part)

        # â‘¡ åœ¨è¿™ä¸ªå­é›†ä¸Šåšâ€œæ—¶åºæ„ŸçŸ¥â€çš„æ’å€¼SMOTE
        X_aug, Y_aug = apply_tsmote_ts(
            X_part, Y_part,
            k_neighbors=5,
            n_synthetic=None,          # é»˜è®¤è¡¥åˆ°ä¸å¤šæ•°ç±»ç›¸åŒ
            alpha_mode="per_timestep", # æ›´æœ‰å¤šæ ·æ€§ï¼›æƒ³æ›´ç¨³ç”¨ "global"
            noise_std=0.0,             # æ•°æ®å·²æ ‡å‡†åŒ–å¯è®¾ 0~0.01
            smooth=False,              # å¦‚éœ€æ›´é¡ºæ»‘å¯è®¾ True
            random_state=random_state
        )
        _print_class_info("tSMOTE step2 (after SMOTE-TS)", Y_aug)
        return X_aug, Y_aug

    elif method == "timegan":
        # â‘  æ¬ é‡‡å¤šæ•°ç±»åˆ° 1:2
        if len(neg_idx) > 2 * len(pos_idx):
            keep_neg = rng.choice(neg_idx, size=2 * len(pos_idx), replace=False)
        else:
            keep_neg = neg_idx
        keep_idx = np.concatenate([pos_idx, keep_neg])
        X_part, Y_part = X[keep_idx], Y[keep_idx]
        _print_class_info("TimeGAN step1 (after 1:2 undersample)", Y_part)

        # â‘¡ TimeGAN è¡¥é½
        X_aug, Y_aug = generate_timegan_samples(
            X_part, Y_part,
            epochs=TG_EPOCHS, lr=TG_LR, Z=TG_Z, H=TG_H, batch=TG_BATCH,
            random_state=random_state
        )
        _print_class_info("TimeGAN step2 (after synth)", Y_aug)
        return X_aug, Y_aug

    else:
        raise ValueError(f"Unknown sampling method: {method}")


# =====================================
# Step 4: Save processed train data to .npy files
# =====================================
def save_data(X, Y, output_dir):
    os.makedirs(output_dir, exist_ok=True)
    np.save(os.path.join(output_dir, 'trainX.npy'), X)
    np.save(os.path.join(output_dir, 'trainY.npy'), Y)
    print(f"âœ… Saved to {output_dir} | Samples: {len(Y)}, Defaults: {int(Y.sum())}, Non-defaults: {len(Y) - int(Y.sum())}")


# =====================================
# Step 5: Main loop for batch processing
# =====================================
if __name__ == "__main__":
    # å›ºå®šéšæœºç§å­ï¼Œä¿è¯å¯å¤ç°
    random.seed(SEED)
    np.random.seed(SEED)

    input_files = [
        "data/processed_data/historical_data_time_2019Q1.csv",
        "data/processed_data/historical_data_time_2019Q2.csv",
        "data/processed_data/historical_data_time_2019Q3.csv",
        "data/processed_data/historical_data_time_2019Q4.csv"
    ]

    master_data_sizes = [9, 12, 15, 18, 21, 24, 27, 30, 33, 36]
    sampling_methods = ["random_undersample"]
    # å¿«é€ŸéªŒè¯å¯ä¸´æ—¶æ”¹æˆï¼š
    # input_files = ["data/processed_data/historical_data_time_2019Q1.csv"]
    # master_data_sizes = [12]
    # sampling_methods = ["timegan"]

    stats_csv_path = "train_data_statistics_summary_2019.csv"
    write_header = not os.path.exists(stats_csv_path)

    first_file = os.path.basename(input_files[0])
    year_str = next((part[:4] for part in first_file.split('_') if part[:4].isdigit()), "Unknown")

    for master_data_size in master_data_sizes:
        for method in sampling_methods:
            all_X, all_Y = [], []
            total_sample_size = 0
            total_num_loans = 0
            total_loan_length = 0
            total_num_defaults = 0

            for input_file in input_files:
                if not os.path.exists(input_file):
                    print(f"âŒ File not found: {input_file}")
                    continue

                # âœ… æ”¯æŒ FRACTION æŠ½æ ·å¿«é€Ÿè·‘
                grouped_data, _ = load_and_process_data(input_file, fraction=FRACTION, seed=SEED)
                X, Y, sample_size, num_loans, loan_length, num_defaults = build_train_data(grouped_data, master_data_size)

                if len(X) == 0:
                    continue

                # åœ¨æ¯ä¸ªå­£åº¦ä¸Šå…ˆåšé‡‡æ ·ï¼Œå†æ±‡æ€»
                sampled_X, sampled_Y = apply_sampling(X, Y, method, random_state=SEED)
                all_X.append(sampled_X)
                all_Y.append(sampled_Y)

                total_sample_size += sample_size
                total_num_loans += num_loans
                total_loan_length += loan_length
                total_num_defaults += num_defaults

            if all_X:
                final_X = np.concatenate(all_X, axis=0)
                final_Y = np.concatenate(all_Y, axis=0)

                # âœ… åŠ å…¥å¹´ä»½åŒºåˆ†
                output_path = f"data/npy_merged_{year_str}_master_data_size_{master_data_size}_{method}"
                save_data(final_X, final_Y, output_path)

                final_num_defaults = int(final_Y.sum())
                final_num_total = len(final_Y)
                default_rate = total_num_defaults / total_num_loans if total_num_loans > 0 else 0.0
                final_default_rate = final_num_defaults / final_num_total if final_num_total > 0 else 0.0
                avg_loan_length = total_loan_length / total_num_loans if total_num_loans > 0 else 0.0
                final_avg_loan_length = master_data_size

                with open(stats_csv_path, mode="a", newline="") as file:
                    writer = csv.writer(file)
                    if write_header:
                        writer.writerow([
                            "year",
                            "master_data_size",
                            "method",
                            "original_sample_size",
                            "original_num_loans",
                            "avg_loan_length",
                            "num_defaults",
                            "default_rate",
                            "final_sample_size",
                            "final_num_loans",
                            "final_avg_loan_length",
                            "final_num_defaults",
                            "final_default_rate"
                        ])
                        write_header = False

                    writer.writerow([
                        year_str,
                        master_data_size,
                        method,
                        total_sample_size,
                        total_num_loans,
                        round(avg_loan_length, 2),
                        total_num_defaults,
                        round(default_rate, 4),
                        final_num_total,
                        final_num_total,
                        final_avg_loan_length,
                        final_num_defaults,
                        round(final_default_rate, 4)
                    ])

                print(f"ğŸ“ˆ Stats saved to {stats_csv_path}")
            else:
                print(f"âš ï¸ No data to save for master_data_size={master_data_size}, method={method}")
